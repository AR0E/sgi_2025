\chapter{Day 2}

\section{Mesh Parametrization}

\begin{definition}[UV Maps]

The idea is we want to "squish" or "flatten" our 3D mesh into a snug
2D projection, an image. This produces a \emph{\textbf{UV map}}, and
the procedure in abstract is to assign a subset of 3D space 
$V$ contained by edges to a domain in 2D space $\Omega$:

\begin{align*}
    U: V\subset \RR^3 \rightarrow \Omega \subset \RR^2
\end{align*}
    
\end{definition}

If the edge is a closed object, like a cube, obviously if you try to squish
it, some bits of the cube will be on top of each other, so a no-go to
a UV map. What can be done is cheating; you cut some bits of the original
3D object first so that it becomes "flattable".

\spa

This procedure of incision is sucessful once we reach a
\emph{disk topology}, containing a single boundary loop. Some meshes
require that we make multiple seams and separate multiple sections into
little "UV islands"; this can be done for semantic or mathematical
purposes. Our objective in parametrization is to minimize
\emph{distortion} and the number of cuts.

\subsection{Springs algorithm (Tutte Embedding)}

A basic mesh parametrization algorithm treats the entire mesh as a
set of springs with different elastic coefficients; we want to
"relax" the whole mesh by allowing the springs to stretch until they're
flat and constrained to some boundary. This is known as a
\emph{\textbf{force-directing algorithm}} for making planar graphs.
A \href{https://www.youtube.com/watch?v=WWm-g2nLHds&list=PLubYOWSl9mIvtnRjCCHP3wqNETTHYjQex}{good playlist.}

\spa

This is the same as trying to minimize the potential energy $U$
of the entire system of springs.

\begin{align*}
\min_{U} \sum_{\{i,j\} \in \bf{E}} w_{ij}\bf{||u_j-u_i||}^2
\approx \min_{\text{Pot. energy}} \sum_{\text{vertices}} k\bf{x}^2
\end{align*}

We can numerically solve the relaxation of springs by looking at a basic
$2k\bf{x}=0$ idea:

\begin{align*}
\frac{\partial U}{\partial\bf{u}_i}=\sum_{\{i\,j\}} 2w_{ij} 
(\bf{u}_i-\bf{u}_j)=0 \approx 2k\bf{x}=0
\end{align*}

The spring potential $k$ represented by the stiffness of each edge
$w_{ij}$ can be adjusted to not be a constant, but rather depend on
something. Following \cite{param1stanford}, we have:

\begin{itemize}
    \item \textbf{Uniform weights:} The stiffness of every spring
    is equal. This relaxes them all into equilateral triangles.

    \item \textbf{Harmonic weights:} Dependant on adjacent angles
    between faces to imitate a conformal transformation. According
    to \emph{Lemma 1} in \cite{minimal1}, it comes about from
    minimizing the total discrete \emph{Dirichlet Energy}.

    \item \textbf{Mean-value weights:} Using barycentric coordinates,
    explained below, we can produce weights that are always positive
    and precise in the linear sense.
\end{itemize}

Following \cite{param2sheffer}

\spa

\subsection{Distortion}

The amount of distortion a mesh has is measured by the Jacobian, which
tells you the total infinitesimal variation in a volume element across
all directions.

\begin{align*}
    J_f = U\Sigma V^T = U
\begin{pmatrix}
\sigma_1 & 0\\
 0 & \sigma_2\\
 0 & 0
\end{pmatrix}
V^T
\end{align*}

We can split this into three cases.

\begin{itemize}
    \item \textbf{Area preserving (Equiareal):} 
    If $\text{det}(J_A^2)=\text{det}(J_B^2)$, this implies
    $\sigma_1 \sigma_2 = 1$.
    
    \item \textbf{Conformal (Equiangular):} If there exists a scalar function
    $\phi(u,v)$ that perfectly offsets the variation of the original
    area, such that $J_A^2 = \phi J_B^2$, this is conformal, and it
    implies $\sigma_1=\sigma_2$.
    
    \item \textbf{Isometric:} If the transformation is both
    conformal and equiareal, then it is isometric, and therefore implies
    $J^2_A = J^2_B$, which implies $\sigma_1=\sigma_2=1$.
\end{itemize}

\href{https://github.com/alecjacobson/geometry-processing-parameterization}{Jaconson's course}

\subsection{Barycentric Interpolation}

As per the original floater article \cite{floater1}, as well
as his more explanatory article \cite{floater2}, the basic idea is that
the coordinates \emph{inside} of a triangle $T$ composed of vertices
$\bf{v_1, v_2}$ and $\bf{v_3}$ are linear superpositions of the
vertices scaled by a ratio of the area of the triangles formed by each
vertex and the total triangle area:


\begin{align*}
\bf{x} &= \lambda_1 \bf{v_1} + \lambda_2 \bf{v_2} 
+ \lambda_3 \bf{v_3} \\
\\
\lambda_i&=
\begin{pmatrix}
\text{Area of} \\
\text{triangle made of} \\
\text{base edge } \bf{v}_i
\end{pmatrix}
\bigg/
\begin{pmatrix}
\text{Total area} \\
\text{of triangle}
\end{pmatrix}
\end{align*}

The construction can actually be generalized to any convex polygon
(altough not at all polygons will produce bijective maps \cite{barycentric1})
but we'll stick to triangles. The original motivation of this
system was to mimic \emph{harmonic maps} by piecewise linear maps, that
is, allow a conformal transformation to be more easily carried out.

\spa

Suppose we had a harmonic function $\Delta f = 0$. If we wish to
approximate $f$ by regarding a boundary condition 
$u(\partial \Omega) = u_0$. We want to produce some triangulation
$\mathcal{T}$ whose values of interior edge lengths and angles
look like a conformal map. This then produces a system of equations
that is exactly that of mean value, or barycentric coordinates.

\spa

For mesh parametrization, we want to follow
the procedure outlined in section 5.4 of \cite{floater2}.
Suppose we have a mesh $\bf{M}(E,V, F)$, where $E$ are its edges,
$V$ its vertices and $F$ its faces. Then the \emph{boundary of the mesh}
$\partial\bf{M}$ produces a \emph{boundary polygon}.

\spa

Our goal is to, as per the Tutte embedding, map the boundary polygon
into another convex polygon fixed in the UV plane, and then make all
other triangles of the mesh be convex combinations of their neighbors
in such a fashion as to fill the UV plane.

\spa

Mean coordinates can also be extended to non-convex polygons.

\section{Texture mapping and optimization}

\subsection{Mapping}

Supposing we express a mesh $\bf{M(u,v)}$ in these UV coordinates,
we can store image data in the UV plane and then map this back to the
original mesh to get colors on our mesh.

\spa

The basic method is a \emph{\textbf{triangle soup}} mapping, which
obliterates connectivity information in return for a completely
isometric mapping to the UV space.

\subsection{Optimization}

\section{Level of Detail (\emph{Nicole Feng})}

There's a great deal of difficulty in rendering models
with a lot of details, due to the computational cost.
This originally motivated artists to produce 
different versions of the same object in different
levels of detail, that is, number of polygons,
quality of textures, to save the performance.

There are several issues with this however. The 
first of course is the sheer volume of work; in
some video games, we have thousands of assets. 
We'd have to manually produce three or four versions
of each asset if we'd like to generate accurate LODs
for each one, which is unfeasible.

Further, one can have many transition artifacts
between the low and high LOD versions of the same
object, known as \emph{pop-in}. beyond
straightforward videogames, any kind of
application that consumes large amount of memory
require LOD systems for efficient computing,
in MRI for example.

It would be of our interest to somehow make this
process automatic and very much seamless.

\begin{itemize}
    - What does detail mean?

\end{itemize}

Taylor expansions and fast multipole methods
can be seen as LODs.

\subsection{1D curves}

Let's suppose we have a polyline
$\gamma(t)$ given by some sequence of points
$S$, which contains the vertex positions 
and the connectivity information.

Let's produce a low detail version of the
polyline. Intuitively, we could remove one 
of the vertices or the edges in some kind 
of choice heuristics in order to make it
have less points whilst preserving its 
geometry. We then need some metric to 
tell us \emph{what} exactly is
a similar curve, an error metric.


Minimizing squared distance

A fairly simple error metric is to determine
the average squared distance from the original
polyline.



We retain information from the original
curve, as reference to further iterations.


Suppose we had a curve $A(t):[0,1] \rightarrow \RR^n$,
and we want to build an \emph{approximate} polyline curve
$B(t)$. We want to make our discrete points have the nearest
look to our original curve $A(t)$. 

\spa

As such, we must choose an \emph{error metric} measuring how
similar the two curves are, and make an algorithm that produces a
curve that minimizes this metric. A basic one is simply to measure
the total \emph{area} separating the approximate and real curve,
and attempt to minimize it.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{images/error1.png}
    \caption{Basic error metric}
\end{figure}

\spa

The \emph{distance} between the polyline and the curve is
defined as the \emph{Chamfer Distance} \cite{chamfer1}
of the two sets of points $ch(A,B)$. One can also use the analogous
\emph{Hausdorff Distance} \cite{chamfer3}; ultimately, both
look at the the minimizing or maximizing, respectively, distances between
the two sets. Along the curve integral one can imagine the Hausdorff
metric produces a \emph{mean} value for the error, whilst the Chamfer
metric produces an absolute amount.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{images/chamfer1.png}
    \caption{Chamfer distance produces,
    among evenly sampled points, circles.
    The nearest points are the radius of
    said circles that touch the curve $A(t)$.}
\end{figure}

Where the element $d(A,B)$ is indeed the Euclidean distance.
It makes sense then that the associated integral
becomes a running integral over the Chamfer distance
of the two curves:

\begin{align*}
    \text{(Total error)} = \int_{[0,1]} ch(A,B) 
\end{align*}

And we'd like to readjust and resample $B(t)$ to
minimize this cost functional. Let us discuss, besides Chamfer, some 
other useful error metrics.

\subsection{Different error distances}

Let $X$ be a compact, smooth submanifold in $\RR^3$ 
that inherits the Euclidean metric from $\R^3$, i.e., a closed
surface (like a sphere or a torus) or surface with boundary (like 
a purse). We consider two measures of the \emph{dissimilarity} between 
two sets and their numerical proxies, and apply these two 
measures for $X$ and its deformed mesh $f(M)$ in particular. The 
two measures of dissimilarity are given below.

\subsubsection{\emph{Gromov-Hausdorff distance}}

The normal \emph{Hausforff distance} is the largest of
all minimum (Chamfer) distances along two curves.
That is, if we were to pick all the little Chamfer
distances across all points, we'd pick the largest one
as our Hausdorff distance:

\begin{align*}
    d_{\mathcal{H}}^Z(A, B)&=\max \left(\sup _{a \in A} 
    d_Z(a, B), \sup _{b \in B} d_Z(b, A)\right) \\
    \\
    &= 
    \max \left (
\begin{pmatrix}
\text{Point in A}\\
\text{called }a \\
\text{nearest to B}
\end{pmatrix},
\begin{pmatrix}
\text{Point in B}\\
\text{called }b \\
\text{nearest to A}
\end{pmatrix}
\right )
\end{align*}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/hausdorff1.png}
    \caption{The Hausdorff length is simply the largest of all
    Chamfer distances between the two curves.}
\end{figure}

The \emph{Gromov-Hausdorff} distance is the infimum
or smallest of all selected Hausdorff distance
along chunks of the curve:

\begin{align*}
    d_{\mathcal{G H}}(X, Y) &=\inf_{Z, g_X, g_Y} 
    d_{\mathcal{H}}^Z(X, Y) \\
\end{align*}

Where $g_X: X \rightarrow Z$ and $g_Y: Y \rightarrow Z$ are 
isometric embeddings into the metric space $Z$.
In this case, the metric of $X$ and the deformed mesh $f(M)$ 
are the same as the metric for $Z=\RR^3$. 

\spa

Thus, $\forall$ isometry $g:X\to f(M)$, $\exists$ isometry 
$\varphi: \RR^3 \to \RR^3$, 
i.e., $\varphi \in \operatorname{Iso}(\RR^3)=\mathbb{E}^3$, 
such that $\varphi$ restricts to $f: \varphi|_X=g$. This similarly holds for isometries from 
$f(M)$ to $X$. Therefore,

\begin{align*}
    d_{\mathcal{G H}}(X, f(M)):=\inf_{\varphi \in \mathbb{E}^3} d_{\mathcal{H}}^{\R^3}(\varphi(X), f(M))
\end{align*}

This approach has the nice property that if the points of the mesh $M_n$ 
with $|M_n|=n$ is i.i.d. uniformly sampled from $X$, $X$ assumed to be $k$ 
dimensional with sectional curvature bounded by $K$, then by *Facundo Mémoli 
and Guillermo Sapiro's paper* section 3.2, we have 

$\mathbb{P}\left(d_{\mathcal{H}}^{\RR^d}\left(\varphi(X), f(M_n)\right)>\delta_n\right) \simeq \frac{1}{\ln n}$ as $n \uparrow \infty$, for $\delta_n \gtrsim \left(\frac{\ln n}{n}\right)^{1 / k}$

and thus $d_{\mathcal{G H}}(X, f(M))$ is probablistically approaching to zero when the meash size becomes larger and larger. 

\spa

A computational framework is also provided, where we have the samples 
$S_m$ with $|S_m|=m$ from $X$ and compute a proxy $d_\mathcal{F}$ 
based upon $S_m$ and $M_n$ (see section 3.1) for 
$d_{\mathcal{G H}}(S, f(M))$.

\subsubsection{\emph{Volume error}}

s generalized from the 2D case ($\sum_{\Delta t_i}\int_{t_i}^{t_{i+1}}|\gamma(t)-\overline{\gamma}(t)|dt$ where $\overline{\gamma}(t)$ is the line segment from $f(t_i)$ to $f(t_{i+1})$) we talked about in our last meeting. But this approach necessarily limits our surface to be those globally parametrized by $u$ and $v$ through $\phi$ (for otherwise we cannot directly tell which part of the surface overs the deformed triangle $f(\triangle)$ and cannot determine the domain of double integration). We may use for example the technique provided in *QuadCover paper*.


\begin{align*}
   d_V(X,M)=\sum_{\triangle \in M}\iint_\triangle |X(u,v)-f(\triangle) (u,v)|dudv 
\end{align*}

To evaluate the integral, we consider a finite sampling $S$ from $X$ and do the summation

\begin{align*}
    d_V(X,M)=\sum_{\triangle \in M}\sum_{p\in \operatorname{Im}(\phi|_\triangle)\cap S} |p-f(p_u,p_v)|\operatorname{vol}_p
\end{align*}

Here comes to our customized choice of the unit volume $\operatorname{vol}_p$. We may consider all of the points $\{(p_u,p_v):p\in \operatorname{Im}(\phi|_\triangle)\cap S\}$ on the triangle $\triangle$ and a nice triangulation of them on $\triangle$ and pick one small subdivided triangle neighboring $(p_u,p_v)$ to compute its area as $\operatorname{vol}_p$. We can also consider weighting all the small subdivided triangles touching $(p_u,p_v)$ and apply normalization to this weighted sum of areas of them as $\operatorname{vol}_p$.

\subsubsection{\emph{Fréchet Distance}}

The \emph{Fréchet Distance} between two curves $A$ and $B$ is given by the following
procedure \cite{frechet1}: first we reparametrize both curves in terms of the same parameter
$t$. So $A$ is given by $\alpha(t)$ and $B$ is given by $\beta(t)$. This means the
"sliding along" the curve by changing $t$ can be fine tuned to have different
speeds along the two curves.

\spa

Then, as we look at $A(\alpha(t))$ and $B(\beta(t))$, we look at the distance
$d(A,B)$ between them. First, we force $t$ to increase monotonically, meaning
there is no "walking backwards" as there can be no decrease in $t$, we can only
"slide along" the curve forwards. This is expressed by the condition that
we take $\max_t$.

\spa

Now, as we're looking the strictly forward slide along of these two curves,
we'll want to look at a particular parameter (or moment in time) $t$ 
in both curves, and only \emph{then} do we ask what is the minimum length
between these two points along the two curves:

\begin{align*}
    Fr(A,B) &= \inf_{\alpha, \beta} \max_{t\in[0,1]} d(A(\alpha(t)), B(\beta(t)))
\end{align*}

We can use an analogy to make sense of this process \cite{frechet2}:
Imagine a man walking his dog on a leash. The man walks on one curve, 
$A$, while the dog follows the other curve, $B$. Both the
man and the dog must move forward ($\max_t$). At any given
instant of time $t$, what is their distance from each other?



The \emph{decision problem} asks: is it possible to walk the curves 
with a leash of given length $\varepsilon$? And the optimization 
problem asks for the shortest possible leash or value of length $\varepsilon$.

\spa

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/frechet1.png}
    \caption{The Fréchet Distance is nothing but the distance between
    points that have the same parametrization.}
\end{figure}

One can construct the \emph{free-space diagram} of the associated Fréchet
distance, which is simply the portion of space where $Fr(A,B)\le d$, that is,
all portions where the Fréchet distance are smaller than some value $d$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/frechet2.png}
    \caption{The \emph{free-space diagram} associated to a 
    8-unit distance contour around the curves and the respective
    red intersection.}
\end{figure}

Now why is this all good? Well, let's take again two curves $A$ and $B$,
but this time we make them all squiggly and intersecting:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/frechet3.png}
    \caption{Two intersecting squiggly curves.}
    \label{fig:enter-label}
\end{figure}

If, based on Chamfer, we kindly ask our algorithm to approximate them, 
he won't bother because \emph{everywhere along the two curves} they have a small
minimum distance, set to set. Not only that, but clearly there are \emph{multiple}
equivalent Chamfer points and more, one point can correspond to entire sections
of the arc.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/frechet4.png}
    \caption{Evaluation of "nearest-point" Chamfer distance along the
    two squiggly curves.}
\end{figure}

The reason Fréchet becomes superior is then obvious; by admitting a parametrization
and therefore forcing a injective map between the curves in the distance computation,
we can get all the \emph{shape} information that was missing, at the cost of
slightly higher compute. A detail is that for closed convex shapes $hf(A,B) = Fr(A,B)$
\cite{frechet3}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/frechet5.png}
    \caption{Proper, parametrized Frenet distance between curves, now
    showcasing that since the parameters are all mismatched, one can get
    shape information.}
\end{figure}

Now, there is an obstacle: as said, the higher compute implicates a higher
amount of time. For simple curves and such this is no issue, but currently
there is no subquadratic algorithm for computing this metric, meaning we
have certain difficulties imagining if we can bring it to 3D.

\spa

Another issue that may be investigated is how to produce non-conflicting
and high fidelity parametrizations $\alpha(t)/\beta(t)$. Ideally one wants a
"uniform", linear \emph{intrinsic} parametrization. For example, if we make
$\alpha(t) \propto \exp t$, we risk bunching up all the points near the beginning
or end of the curve.


\subsection{Polyline algorithm}

Once again, we have a curve $A(t)$ that we'd like to approximate
by a polyline $B(t)$; we'll have data composed entirely of simply connected
points in the matrix $\bf{V}$ with some "direction information" 
stored in the order $\bf{v_i\rightarrow v_{i+1}}$ of rows.

\spa

If our space is $n-$dimensional we'd have $n$ columns to 
$\bf{V}$, for now we'll only worry about 3, so we have a
$\bf{V}$ of size $(k \times 3)$, where $k$ is the number of
vertices for $B(t)$.



\subsection{2D surface examples}

We once again have a triplet $(\mathbf{M,E,V})$
containing the vertex positions and connectivity
information. We can extend the same quadric
error metric approach to the surface.


\subsection{Mapping between levels}

\emph{Continous LOD} generates a data structure 
to interpolate between different LODs in such a way
that it looks seamless. 

\begin{align}
    \sum \mathcal{O}
\end{align}

Whenever we do a simplification to a mesh
containing a texture, we need to update the
colors in a way that doesn't distort the colors.

\subsubsection{Multigrid/resolution methods}


