\chapter{Day 4}

\section{Mesh Simplification and Level of Detail}

Meshes contain a lot of geometric information for rendering processes,
and depending on the distance or size of said mesh, it's impractical to keep
the same number of vertices of a mesh in a close-up state and a far-away state.
This is solved by a \emph{LOD} system that transitions from larger to lower
number of polygons the further away or the less visible a mesh is.

\spa

Similarly, in the 1980s for example, methods that produced medical imaging
generated meshes with thousands of polygons, obviously overloading the
processing power at the time; it is then of our interest to somehow diminish
the amount of memory and processing required by lowering the number of triangles,
whilst keeping the shape and general details the same. So we have three major
types of mesh simplification \cite{realtime}:

\begin{itemize}
    \item \textbf{Static:} Static simplification is the idea of 
    creating separate level of detail (LOD) models before rendering begins, 
    and the renderer chooses among these.
    
    \item \textbf{Dynamic:} Gives a continuous spectrum of
    LOD models instead of a few discrete models, and so such methods 
    are referred to as continuous level of detail (CLOD) algorithms
    
    \item \textbf{View-dependant:} Where the level of
    detail varies within the model.
\end{itemize}

\spa

Two major methods were introduced to solve this at the time:
\textbf{\emph{global}} and\textbf{\emph{local}} methods for
LOD. The global method was explored by \cite{Hoppe1} and \cite{turk1}, 
whilst local methods were mostly developed by \cite{schroeder1}. 
However, many methods have been further debveloped over the years, and
many error metrics associated to the production of the coarser meshes
have arisen.

\spa

For example, rather than seeing how crooked the coarse mesh is
by some geometric argument, you can use the total screen-space
deviation of the original compared to the new mesh, as we would
only care about what we see in our screens.

\section{Dynamic simplification and Quadric Error Metric}

Developed by \emph{Pixar} in \cite{LOD1}, the idea is to produce a
sucessive series of \emph{edge collapses} to reduce the total number
of vertices and create a simpler model. The edge collapse procedure
takes two vertices $\bf{(v_1,v_2)}$ and maps them both to a new
vertex $\bf{v_3}$. This is usually written 
$\bf{(v_1,v_2)} \xrightarrow{\text{collapse}} \bf{v_3}$, and
removes one vertex, three edges and two faces.

\spa

The first question is where to put this new vertex $\bf{v}$; the answer
was to produce a cost function $Q$ based on the signed distance from the nearest
points of the vertex and the plane: a \emph{quadric error} is the smallest squared distance from a point to planes, so we have:

\begin{align*}
    Q &= \sum_i \ (\bf{n}_i \cdot \bf{v} + d_i )^2 \\
    &= \sum_i 
\left [ \begin{pmatrix}
\text{Plane} \\
\text{normal}
\end{pmatrix}
\cdot
\begin{pmatrix}
\text{New vertex} \\
\text{position}
\end{pmatrix} +
\begin{pmatrix}
\text{Plane offset} \\
\text{from origin}
\end{pmatrix} \right ]^2
\end{align*}

And we want to minimize this $Q$ to find $\bf{v}$.
Derek Liu's explanation of this went like this; first we
express this $Q$ in terms of matrices such that we implement it
in a linear solver. Ignore the $d_i$ correction term, and express
$\bf{v}$ as $\bf{(x-p)}$, where $\bf{p}$ is in the plane
and $\bf{x}$ is outside it:

\begin{align*}
    Q(\bf{x}) &= (\bf{x-p\cdot n})^2 \\
    &= \left \langle \bf{(x-p)n} \right \rangle^2 \\
    &=\left \langle \bf{(xn-pn)} \right \rangle^2 \\
    &=\left \langle \bf{(x\cdot n-p\cdot n)} \right \rangle^2 \\
    &=\bf{(xn)^2- 2(x\cdot n)(p\cdot n) + (pn)^2}
\end{align*}

Now we transfer from geometric algebra to matrix notation:

\begin{align*}
    Q(\bf{x}) &= \bf{(n^Tx)^2- 2(n^Tx)(n^Tp) + (n^Tp)^2} \\
&= \bf{(n^Tx)^T(n^Tx)- 2(n^Tx)(n^Tp) + (n^Tp)^T(n^Tp)} \\
&= \bf{(x^Tnn^Tx)- 2(nn^Tp)x + p^Tnn^Tp} 
\end{align*}

We can then simplify $\bf{nn^T = A, (nn^Tp)^T = b}$ and
$\bf{p^Tnn^Tp = c}$. This produces a quadratic equation
$Q(\bf{x}) = \bf{x^Tax + 2b^Tx + c}$. To \emph{minimize}
this function we only take the derivative and equal it to zero,
yielding a single linear solve for us to handle. When solved,
this gives us the $\bf{x}$, or in the original cost,
$\bf{v}$, that gives us the new simplified edge.

\spa

According to section 5.1 of \cite{LOD1}, the matrix $\bf{Q}$
used for finding optimal vertex positions will be invertible as long as the
level surfaces of the \emph{quadrics} generated by the quadratic equation
in plane space, are non-degenerate ellipsoids. In this case,
$\bf{v}$, our desired vertex, will be at the center of the ellipsoid.

\spa

There is a fairly intuitive interpretation \cite{LOD2}; the inner product will measure the cost
based on the "normal like" projected component of $\bf{x}$ onto $\bf{n}$;
within a concentric circle centered around the axis of $\bf{n}$, all $\bf{x}$
have the same projection. Changing the projection value produces an ellipsoid for all
collections of concentric circles. Indeed the surface of constant values of $Q$
measures the set of all points with a fixed error.

\section{Other Quadric Error Metrics}

There are many other of these quadric error metrics that have been
developed over the years. For example, Heckbert and Garland
immediately designed another quadric algorithm \cite{LOD2}
that took into consideration not only geometric data, but also
texture and color information contained in the vertices.

\spa

The usual error metric $Q(\bf{x})$ is dependant on
$\bf{x}=[x\ y\ z]^T$, we can simply make a higher dimensional
vector that also encodes RGB information $[x\ y\ z\ r\ g\ b]^T$.
Then we simply have to make a higher dimensional distance function
and do the exact same procedure. If we want \emph{more} information,
such as textures, normal maps and the works, we just have to add more
elements to this vector. The issue with this is that the matrix $Q$
grows quadratically, and so becomes large, having 121 coefficients to
solve for if we have 11 variables.

\spa

We can extend the same sort of idea further and get better results
\cite{LOD3}, but more interesting is to check some other ideas.
In \cite{LOD4}, multiple methods of geometrical simplification
are discussed, including \emph{volume} and \emph{boundary preservation}
and \emph{optimization}, as well as \emph{triangle shape optimization.}

\spa

Modern methods are very sophisticated, such as the one discussed
in \cite{LOD5} of \emph{probabilistic quadrics}, are able to do well
in cases of near collinear faces and other highly dense meshes by
developing quadrics whose outputs are expectation values of where
the approximate vertex \emph{ought} to be.

\spa

Another one, discussed in \cite{LOD6}, attemepts to conserve the 
eigenvalues of the Laplace-Beltrami operator $\Delta f$. This one 
merits further investigation.

\section{Sucessive Parametrization}

As we sucessively decimate meshes into coarser and coarser states,
the relative coordinate grid associated to the fine mesh is also
lost; that is, we need a way to keep track of the positions of
features on top of the mesh.

\spa

Way back in the day \cite{LOD2} was already aware of this issue,
but let us suppose, as before, that we want to simplify a mesh with
texture data on top; the textures that correspond to the eyes must
geometrically stay in the same position as they did in the finer mesh,
only occupying different positions inside the triangles.

\spa

Another simple use case is that we may perhaps want to use the mesh
content to physically simulate something, as a cloth interaction
or a PDE on top of the mesh. With loads of triangles this is expensive,
but we can decimate the mesh, simulate, then return these values to the
finer mesh, with an accuracy blow but at the reward of a much smaller
compute time.

\spa

So we have the need to develop a mathematical correspondence between
coarse and fine coordinate systems. To do this we will develop
parametrization algorithms

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{images/param1.png}
    \caption{Assigning a bijective coordinate map to coarse mesh
    from fine mesh}
\end{figure}

Following \cite{param1} \cite{param2},

\section{Multigrid Solvers}

Basic linear solvers of the form $\bf{Ax = v}$

\begin{itemize}
    \item \textbf{Relaxation 1}
    \item \textbf{Restriction}
    \item \textbf{Direct solve}
    \item \textbf{Prolongation}
    \item \textbf{Relaxation 2}
\end{itemize}

Garlekin multigrid introduces a \emph{prolongation matrix} $\bf{P}$